# -*- coding: utf-8 -*-
"""Actividad 11-OK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sIeuOEATKPaLBHHpnmSOc3FPIvFVrBLh

1.
 *   Usando Socrata, Cargue un dataset que se encuentre en el portal de datos públicos de Colombia

*   Realice las tareas de limpieza necesarias
*   Dividir el conjunto entre datos de entrenamiento y datos de prueba, los datos de prueba serán el 15% del dataset
*   Aplicar el algoritmo de clasificación de los K vecinos más cercanos
*   Calcular los puntajes de la clasificación
*   Concluir si la clasificación fue buena o no
*   Realice las tareas de limpieza necesarias
*   Aplicar el algoritmo de agrupamiento de las K medias
*  Calcular los puntajes de la agrupación
*  Representar gráficamente los grupos
* Concluir si la agrupación fue buena o no
"""

import pandas as pd
import numpy as np
#Importar dataset como un DataFrame
diabetes_df = pd.read_csv("diabetes_012_health_indicators_BRFSS2015.csv", delimiter=",")

#Se almacenan los datos de entrada y de salida
diabetes_df.info()

#se visualizan todas las columanas de un DataFrame
pd.options.display.max_columns = None

diabetes_df.head(10)
#Mostrar los primeros 10 registros del DataFrame

print(diabetes_df.head(10))

##Limpieza de datos
##Eliminacion de la columna Education y de la columna Income

diabetes_df = diabetes_df.drop('Education',axis=1)
diabetes_df = diabetes_df.drop('Income',axis=1)

diabetes_df.info()

#Se trabaja con una copia del Dataframe
diakv_df=diabetes_df.copy()

#Eliminar nulos
diakv_df= diakv_df.dropna(axis=0,how="any")

diakv_df.info()

"""1. Guardar la salida en un arreglo
2. Guardar las variables predictoras (datos de entrada) en una matriz

# Pasar a **numerico**
"""

#Transformacion de los datos de salida de la clasificación en un arreglo de numpy
salida=diakv_df['Diabetes_012']

#salida=pd.to_numeric(diakv_df['Diabetes_012'], errors='coerce')
salidas = pd.to_numeric(diakv_df['Diabetes_012'],errors='coerce')

#print(salida)
y=np.array(salida)
print(y)

# Se transforman los datos de entrada en un arreglo de numpy
#Se crean arreglos de numpy

#print(diakv_df.columns)

#lista de los nombres de las columnas del dataframe
listNom = diakv_df.columns.tolist()

print(listNom)
#Quitar el primer elemento de la lista (la salida, el diagnostico)
listNom.pop(0)
print(listNom)

#constuir lista de los arreglos de los atributos(columnas)
listx=[]
for column in listNom:
  x=np.array(diakv_df[column])
  #x=np.array(pd.to_numeric(diakv_df[column],errors='coerce))
  listx.append(x)

print(len(listx[0]))

Salida=[]
for i in range(len(listx[0])):
  aux=[]
  for j in range(len(listNom)):
    aux.append(listx[j][i])
  Salida.append(aux)
X=np.array(Salida)

print(X)

#Armamos el modelo. De clasificación o de agrupamiento
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=123,stratify=y)

print(X_test)
print(y_test)

#Crear mnodelo de k vecinos
#Modelo de clasificación
Knn=KNeighborsClassifier(n_neighbors=3)
Knn.fit(X_train,y_train)

#Revisar la metrica
Knn.score(X_test,y_test)

predicTest=Knn.predict(X_test)

print(predicTest.tolist())
print(y_test.tolist())

print(len(predicTest.tolist()))

"""Concluimos que la clasificación fue buena, ya que el puntaje que se logró fue superior al 50%, obteniendo un 82% de aciertos logrados en el modelo de precisión respecto a los datos reales de salida. Aunque podemos ver que tiene algunos inconvenientes relevantes:

1. En los datos reales hay varios que son positivos para diabetes (2.0), pero, en la predicción los tomó como (0,0) que es sin diabetes o solo en el embarazo, osea, un falso negativo.

2. Así como también en el real dice que es 0.0, que indica sin diabetes o solo durante el embarazo, y en la predicción dice que 2.0, que indica que tiene diabetes, osea, un falso positivo.

La información anterior afecta la certeza de la clasificación en ambos casos, esto  implica que en los casos en los que arroja un falso positivo, es decir, que le está diciendo a una persona que no tiene diabetes que si la tiene, podría traer afectaciones reales a la salud de los pacientes si solo se confía en esta predicción, si no se realizan más estudios para verificar el resultado obtenido, debido a que  se podría medicar a un paciente erróneamente, lo que sería muy grave; y en los casos en que arroja un falso negativo, cuando dice que no tiene diabetes pero la persona si la tiene, es también muy grave, porque, al contrario del caso anterior, una persona con diabetes puede no ser tratada, y, por ende, no medicada, ni vigilado su estado de salud, trayendo consecuencias nefastas en ambos casos.

Una manera de poder subsanar esta situación es aportando más información acerca de población con diabetes, ya que verificamos la cantidad de los datos arrojados en el K medias, hay muy poca información para los casos de pacientes con diabetes. En conclusión, agregando información se podría solucionar este inconveniente.

## **Se realiza el agrupamiento K medias**
"""

#Visualizacion de todas las columnas del DataFrame
pd.options.display.max_columns = None

#Se construye un DataFrame para ordenar los valores por la salida (variable clase)
import pandas as pd
import numpy as np

# 1ero. se añaden las variables predictoras (entradas), usando X

#Usamos la lista de los nombres de las columnas para asifnarlos al DataFrame
listNom
# Convertir la matriz en un DataFrame
diabetes_df2 = pd.DataFrame(X,columns=listNom)

diabetes_df2

#Añadimos la columna del diagnostico (la variable clase) la calificacion

#Usando y
#Agregar la nueva columna al DataFrame
diabetes_df2['Diagnostic'] = y

#Mostramos el DataFrame y comprobamos
diabetes_df2

#Se ordena el dataframe dada la clase (0 es No diabetes, 1 es prediabetes, 2 es Diabetes)

#Para ordenar el dataframe usamos el método de ordenación
#Actualizamos el dataframe
diabetes_df2 = diabetes_df2.sort_values('Diagnostic')

#Reseteamos los indices
diabetes_df2.reset_index(inplace=True, drop=True)

diabetes_df2

#Aqui podemos ver en rango estan los valores de 1
diabetes_df2[diabetes_df2['Diagnostic']==1]

#Vamos a mostrar cuantos registros hay por grupo

cond0 = diabetes_df2['Diagnostic']==0
grupo0 = sum(cond0)
print(grupo0)

cond1 = diabetes_df2['Diagnostic']==1
grupo1 = sum(cond1)
print(grupo1)

cond2= diabetes_df2['Diagnostic']==2
grupo2 = sum(cond2)
print(grupo2)

print(grupo0+grupo1+grupo2)

#Vamos a mostrar en un arreglo cómo se ve la distribución de los grupos
diag = pd.to_numeric(diabetes_df2['Diagnostic'],errors='coerce')
diag=np.array(diag)
print(diag)

#Debido a la dimensionalidad del arreglo lo mejor es realizar una grafica
#Importamos matplotlib
import matplotlib.pyplot as plt
plt.scatter(range(len(diabetes_df2['Diagnostic'])),diabetes_df2['Diagnostic'])
plt.show()

"""Se observa en la gráfica que desde el registro 0 al 211.000 aproximadamente, todos son 0.0, lo que significa sin diabetes o solo durante el embarazo, que una pequeña porción es 1.0, que es prediabetes, y 2.0 que son con diabetes, que en comparación es una porción más significativa que los prediabéticos. Es importante anotar que, aunque aparecen datos decimales no aparecen datos entre medias, ya que solo nos interesan los 0.0 = Sin diabetes o solo durante el embarazo, 1.0 = Prediabetes y 2.0 = Diabéticos."""

#Se quita la columna de la clase (diagnostico), para que algoritmo encuentre esa clasificación
diabe_df3 = diabetes_df2.drop('Diagnostic', axis=1)

#Un ejercicio que podemos realizar es revisar cual es la mejor cantidad de grupos para el algortimo
inercias = []
for i in range(1,10):
  Kmeans=KMeans(n_clusters=i,max_iter=300)
  Kmeans.fit(diabe_df3)
  inercias.append(Kmeans.inertia_)

plt.plot(range(1,10),inercias)

"""La grafica lo que nos indica es la efectividad que tuvo la cantidad de cluster en la agrupación, en el eje “Y” vemos la cantidad de cluster, y en eje “X” la efectividad. La gráfica nos muestra que, entre menos, ya no es tan efectivo, osea, que cuando llega a 9 deja de tener efecto, lo que quiere decir que, al agregar más cluster, no nos va a mejorar el agrupamiento. Aunque nos debe interesar el punto de inflexión o codo, que nos muestra en qué instante la gráfica comienza a tener un comportamiento horizontal; para el caso, lo vemos en 3.0."""

#Ahora se aplicara el algortimo de K Medias para buscar los posibles grupos

#Se debe quitar la columna de salida porque se necesitan solo los datos de entrada
from sklearn.cluster import KMeans
Kmeans=KMeans(n_clusters=3,max_iter=300)
Kmeans.fit(diabe_df3)

#Ahora revisaremos los resultados
objetivos=Kmeans.fit_predict(diabe_df3)
print(objetivos)

#Debido a que los datos están ordenados podemos contrastar si el agrupamiento es parecido al real

#Graficaremos el vector resultante
plt.scatter(range(len(diabe_df3['Age'])),objetivos)
plt.show()
plt.close()

#Vamos a ordenar y contar cuantos elementos hay en cada cluster
obje_ord=np.sort(objetivos)
print(obje_ord)

plt.scatter(range(len(obje_ord)),obje_ord)
plt.show()

"""Concluimos que, al ordenar el arreglo de objetivos, que es el resultado de la predicción, lo podemos ordenar y graficar de nuevo, y observamos que presenta un comportamiento similar a la inicial, solo que pudo clasificarse con otras características."""

#Modelo usando BDSCAN
from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(diabetes_df2)

Diagnostic=Kmeans.fit_predict(diabetes_df2)
print()

dbscan.labels_